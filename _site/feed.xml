<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>James&apos; Portfolio</title>
    <description>Northwestern MS in Robotics Student Portfolio</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Sun, 25 Dec 2022 18:07:56 -0600</pubDate>
    <lastBuildDate>Sun, 25 Dec 2022 18:07:56 -0600</lastBuildDate>
    <generator>Jekyll v3.9.0</generator>
    
      <item>
        <title>Motion Mirroring Robotic Arm</title>
        <description>&lt;p&gt;C, Mechatronics, Embedded Systems, I2C&lt;/p&gt;

&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;
&lt;p&gt;As part of Northwestern’s microprocessor design course, my group and I created an Electromyography
(EMG) controlled robotic manipulator. This allows a user to connect an EMG sensor to their arm to
open and close a robotic gripper by simply opening and closing their hand. We also combined project with 
another group in the class who were controlling a 2 DOF robotic arm’s motion based off of IMU data.
By combining these projects, we were able to create a system that would move a robot arm and control 
the gripper based off of the user’s motion.&lt;/p&gt;

&lt;h2 id=&quot;emg-gripper&quot;&gt;EMG Gripper&lt;/h2&gt;

&lt;div align=&quot;center&quot;&gt;&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/6Y9bxQ33sTY&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;

&lt;p&gt;This project was developed around the Microbit V2. Code is typically written to the Microbit in Python,
however, we were writing directly to the Microbit’s microcontroller (nRF52833) in C. The end-effector
itself is a mechanical gripper that opens and closes based off of the position of a connected servo.
We controlled this servo with a PCA9685 servo driver over I2C. Additionally, we used two sensors
in this project: an EMG sensor and a force sensitive resistor (FSR). The EMG sensor measures electrical 
signals in muscles that are generated from movement. This is attached to the users arm with electrodes
and used to detect if the hand is closed or not. The FSR measures the amount of
force that is applied to it. This is placed on the inside of the gripper and used to detect if the 
gripper is actively gripping something.&lt;/p&gt;

&lt;p&gt;The basic operation of this system is that the output of the two sensors are constantly being read
by the Microbit. If the EMG sensor gives a high signal, then the user is trying to grasp. When this
is detected, PWM signals are sent to the servo driver over I2C in a loop, closing the gripper incrementally
with each iteration. When an item is grasped, a forced is applied to the FSR, producing a 
high signal that is used to break the grasping loop and hold the gripper at a constant position.
Once, the user releases their grasp, the gripper will then start to incrementally open, unless
interrupted by another grasp command.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;&lt;h2&gt; &lt;a href=&quot;https://github.com/oubrejames/emgripper&quot;&gt;View EMG Gripper Github&lt;/a&gt;&lt;/h2&gt;&lt;/div&gt;

&lt;h2 id=&quot;emg-gripper--imu-arm&quot;&gt;EMG Gripper + IMU Arm&lt;/h2&gt;

&lt;div align=&quot;center&quot;&gt;&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/xjytBSXibu4&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;

&lt;p&gt;When the EMG gripper is combined with the IMU controlled arm, the resulting system allows a user to
manipulate the robot to perform various actions just by moving their own arm and hand.&lt;/p&gt;

&lt;h2 id=&quot;the-team&quot;&gt;The Team&lt;/h2&gt;
&lt;h4 id=&quot;emg-gripper-1&quot;&gt;EMG Gripper&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;David Dorf&lt;/li&gt;
  &lt;li&gt;Katie Hughes&lt;/li&gt;
  &lt;li&gt;James Oubre&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;imu-arm&quot;&gt;IMU Arm&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;Nicolas Morales&lt;/li&gt;
  &lt;li&gt;Hang Yin&lt;/li&gt;
  &lt;li&gt;Felipe Jannarone&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Sun, 11 Dec 2022 00:00:00 -0600</pubDate>
        <link>http://localhost:4000/Gripper/</link>
        <guid isPermaLink="true">http://localhost:4000/Gripper/</guid>
        
        
        <category>C</category>
        
        <category>Mechatronics</category>
        
        <category>Embedded Systems</category>
        
        <category>I2C</category>
        
      </item>
    
      <item>
        <title>BotChocolate</title>
        <description>&lt;p&gt;Computer Vision, OpenCV, ROS2, Python, Motion Planning, Moveit, Intel Realsense, Emika Franka Robot Arm&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;&lt;iframe width=&quot;720&quot; height=&quot;400&quot; src=&quot;https://www.youtube.com/embed/Q_aNWWe4h5M&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;

&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;
&lt;p&gt;The purpose of this project was to create hot chocolate using the Franka Emika 7 DOF robot arm. To perceive its environment, the system utilizes an Intel D435i camera AprilTags. Upon initial setup, a calibration process must be completed. After this, the process consisted of using AprilTags to locate a scoop of cocoa, a mug, a spoon, and a kettle relative to the robot. Next, using our custom MoveIt API for Python, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;movebot&lt;/code&gt;, the robot is able to perform path planning between all of these objects. It turns on the kettle, dumps the cocoa powder into the mug, pours the hot water over the power, and stirs the mixture it with the spoon.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;&lt;h2&gt; &lt;a href=&quot;https://github.com/oubrejames/bot_chocolate&quot;&gt;View BotChocolate Github&lt;/a&gt;&lt;/h2&gt;&lt;/div&gt;

&lt;h2 id=&quot;moveit-api&quot;&gt;Moveit API&lt;/h2&gt;
&lt;p&gt;Because the ROS2 MoveIt package does not have a Python API yet, creating one was the first step. By 
looking into the MoveIt ROS2 actions and services, we were able to create an API that takes either 
Cartesian coordinates of the Franka end-efffector or joint-state positions of each joint and creates
a path to the specified position. It also allows for the choice of moving in a Cartesian path.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/Sed9XwHT-7c&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;

&lt;h2 id=&quot;vision&quot;&gt;Vision&lt;/h2&gt;
&lt;p&gt;The vision system is comprised of two major parts: calibration and component location. AprilTags are
used to detect where each of the hot chocolate making components are. The AprilTags are located using
the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;apriltag_ros&lt;/code&gt; package.&lt;/p&gt;

&lt;p&gt;There is one tag fixed to a kettle and one tag fixed to a jig that has slots for a mug, cocoa scooper,
and a spoon. Once these tags are found in the camera frame, there are two transformation trees 
describing where all the hot chocolate components are relative to the camera and where each link of
the Franka robot arm is relative to the robot’s base. However, we need to connect these trees to 
get the location of each component relative to the robot’s base. This is where the calibration is 
used. An AprilTag must be placed in the robot’s gripper, aligned with the gripper’s
coordinate frame and in view of the camera to calibrate. We then wrote a program that finds the location of the tag
and relates it to the position of the base, connecting the robot frame to the camera frame. We then
save this transformation to a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.yaml&lt;/code&gt; file to be used later on when running the main hot chocolate
making sequence.&lt;/p&gt;

&lt;p&gt;After obtaining the transformation from the camera to the robot, the robot can now use the locations
of the two AprilTags to locate and manipulate the hot chocolate components as necessary.&lt;/p&gt;

&lt;h2 id=&quot;motion-planning&quot;&gt;Motion Planning&lt;/h2&gt;
&lt;p&gt;Once the locations of all the components are known, it’s time to make hot chocolate. Cartesian paths
are used to move the robot in straight lines and rotational path planning is used for achieving motions
like tilting to grab the scoop and dumping the cocoa. Both Cartesian and rotational motion are used to achieve pouring.
To make a cup of hot chocolate, the robot first turns on the kettle to heat the water, grab the cocoa scoop, 
pours it in a mug, pours the hot water over the powder, and stirs. We also used serval intermediate 
waypoints to help avoid reaching the joint limits of the robot.&lt;/p&gt;

&lt;h2 id=&quot;the-team&quot;&gt;The Team&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Shantao Cao&lt;/li&gt;
  &lt;li&gt;Allan Garcia-Casal&lt;/li&gt;
  &lt;li&gt;Nicholas Marks&lt;/li&gt;
  &lt;li&gt;James Oubre&lt;/li&gt;
  &lt;li&gt;David Dorf&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46512429/206768445-4503edc2-2075-48b4-baf7-e6dc7bd3ca86.png&quot; alt=&quot;bot_choc-min&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Fri, 09 Dec 2022 00:00:00 -0600</pubDate>
        <link>http://localhost:4000/Botchoc/</link>
        <guid isPermaLink="true">http://localhost:4000/Botchoc/</guid>
        
        
        <category>Computer Vision</category>
        
        <category>OpenCV</category>
        
        <category>ROS2</category>
        
        <category>Python</category>
        
        <category>Motion Planning</category>
        
        <category>Moveit</category>
        
        <category>Intel Realsense</category>
        
        <category>Emika Franka Robot Arm</category>
        
      </item>
    
      <item>
        <title>Physics Simulation</title>
        <description>&lt;p&gt;Python, Dynamic Systems, Simulation, Jupyter Notebook&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/oubrejames/oubrejames.github.io/4b9a9bb1b282e894d9406c93f0adbf26e34a60b5/assets/images/cropped_jack.gif&quot; alt=&quot;jack_box_gif&quot; width=&quot;600&quot; /&gt;&lt;/div&gt;

&lt;p&gt;Using Lagrangian dynamics, I was able to simulate a box bouncing around within a larger box. First,
I calculated the potential and kinetic energies of the system to obtain its Lagrangian and later on,
the Euler-Lagrange equations. These equations describe the dynamics of the system while not
experiencing impact. To detect impact the following coordinate frames were used.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46512429/208279053-1a1dd404-148d-4dbd-aadc-0b7c9d14c366.png&quot; alt=&quot;jack_box_gif&quot; width=&quot;500&quot; /&gt;&lt;/div&gt;

&lt;p&gt;Using the transformation matrices between these frames, I am able to detect impact if any of the 
smaller box’s vertices intersect the larger box’s walls. Each vertex is checked against each wall 
for every time step of the simulation. If an impact is detected, the dynamic equations are updated and
the simulation continues.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;&lt;h2&gt; &lt;a href=&quot;https://github.com/oubrejames/physics-simulator&quot;&gt;View it on Github&lt;/a&gt;&lt;/h2&gt;&lt;/div&gt;
</description>
        <pubDate>Sun, 04 Dec 2022 00:00:00 -0600</pubDate>
        <link>http://localhost:4000/physics_simulator/</link>
        <guid isPermaLink="true">http://localhost:4000/physics_simulator/</guid>
        
        
        <category>Python</category>
        
        <category>Dynamic Systems</category>
        
        <category>Simulation</category>
        
        <category>Jupyter Notebook</category>
        
      </item>
    
      <item>
        <title>Mobile Manipulator Simulation</title>
        <description>&lt;p&gt;Mobile Manipulation / CoppeliaSim / PID Control / Python&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/QY0E-IW8qvQ&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;

&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;
&lt;p&gt;As the capstone project for Northwestern’s Robotic Manipulation course I simulated a KUKA youBot
moving to a cube, grasping it, and then placing it in a desired position. First, I created a function
to generate trajectories from the robot’s starting position to all the different waypoints to 
complete the task. The homogeneous transformation matrices between waypoints are used to create a 
screw or Cartesian trajectories between points. These trajectories are then stored in a matrix representing 
the desired movement of the robot’s chassis and end effector. Next, I implemented a PID feedback
controller based off of the robot’s current position and odometry to ensure that the robot can self
correct if error is introduced. Lastly, the actual positions of the robot are stored in a CSV file
and used to simulate the robot in Coppeliasim.&lt;/p&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;
&lt;p&gt;The simulation completed the task in three different scenarios: with poor PID gains, tuned
PID gains, and a different start and end configuration for the cube.  Overall, each 
simulation is able to reduce to zero error before the robot gets the the cube’s standoff position.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;&lt;h2&gt; &lt;a href=&quot;https://github.com/oubrejames/youbot_simulation&quot;&gt;View it on Github&lt;/a&gt;&lt;/h2&gt;&lt;/div&gt;
</description>
        <pubDate>Thu, 01 Dec 2022 00:00:00 -0600</pubDate>
        <link>http://localhost:4000/robman/</link>
        <guid isPermaLink="true">http://localhost:4000/robman/</guid>
        
        
        <category>Mobile Manipulation / CoppeliaSim / PID Control / Python</category>
        
      </item>
    
      <item>
        <title>Pen Stealing Robot</title>
        <description>&lt;p&gt;Computer Vision, OpenCV, Python, PincherX 100, Intel Realsense&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/JoxrQ2MmBp4&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;

&lt;p&gt;As part of Northwestern’s orientation hackathon, I used an Intel Realsense D435i camera to detect
the location of a purple Northwestern pen and grab it with a Trossen PincherX 100 robot arm. 
To detect the pen, I converted the RGB image from the Realsense to an HSV image and used the HSV values
to find all purple pixels. Next, I created a binary map where the detected pixels were shown as 
white and all other pixels black. Using this and OpenCv’s contour detection I found the pixel location
of the centroid of pen. This allowed me to find the location of the pen relative to the camera using 
the aligned depth image generated from the Realsense. I then convert the pen position from the camera’s
frame to the robot’s frame and control the robot to move to the pen and grasp it.&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;&lt;h2&gt; &lt;a href=&quot;https://github.com/oubrejames/pen_challenge&quot;&gt;View it on Github&lt;/a&gt;&lt;/h2&gt;&lt;/div&gt;

</description>
        <pubDate>Thu, 15 Sep 2022 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/Pen/</link>
        <guid isPermaLink="true">http://localhost:4000/Pen/</guid>
        
        
        <category>Computer Vision</category>
        
        <category>OpenCV</category>
        
        <category>Python</category>
        
        <category>PincherX 100</category>
        
        <category>Intel Realsense</category>
        
      </item>
    
      <item>
        <title>Reasearch</title>
        <description>&lt;p&gt;Computer Vision, OpenCV, UR5 Robot Arm, Manufacturing, Research, Intel Realsense&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;&lt;iframe width=&quot;1080&quot; height=&quot;620&quot; src=&quot;https://www.youtube.com/embed/gyQQtJ2jm6k&quot; title=&quot;robotic_sanding&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;

&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;
&lt;p&gt;While getting my bachelor’s degree at Louisiana State University I worked as an undergraduate researcher
in the Innovation in Control and Robotics Engineering Lab (iCORE Lab). My main area of research was
computer vision for robotic systems. Specifically, as part of an NSF funded research project, I
created a computer vision system to detect surface defects in fiber glass for autonomous sanding.
Once, the locations of the defects were found, two types of path planning were used to create a sanding
path. The path waypoints were then relayed to a UR5e 6 DOF robot arm with a sander attachment to 
sand the defected areas.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46512429/208994781-0e1e5a9e-4540-45f4-ad3e-13374b4bca35.png&quot; /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;System Pipeline&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46512429/208988986-65a70827-1d47-4bcf-aab6-9a6e2c07246c.png&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;System Components&lt;/p&gt;

&lt;!-- &lt;div align=&quot;center&quot;&gt;&lt;h2&gt; &lt;a href=&quot;https://github.com/oubrejames/physics-simulator&quot;&gt;View it on Github&lt;/a&gt;&lt;/h2&gt;&lt;/div&gt; --&gt;
&lt;div align=&quot;center&quot;&gt;&lt;h2&gt; &lt;a href=&quot;https://www.sciencedirect.com/science/article/pii/S240589632201031X/pdf?md5=6790ab4229b08c78bb91bb6a34d5e885&amp;amp;pid=1-s2.0-S240589632201031X-main.pdf&quot;&gt;Read the paper&lt;/a&gt;&lt;/h2&gt;&lt;/div&gt;

&lt;h2 id=&quot;vision&quot;&gt;Vision&lt;/h2&gt;
&lt;p&gt;Using traditional computer vision techniques like canny edge detection, morphological closing, 
contour detection, and binary mapping I was able to detect and isolate defects present on the surface
of a fiber glass panel.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46512429/208990411-230117af-4a57-4276-bc6e-0b72b197a123.png&quot; /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;Computer Vision Pipeline&lt;/p&gt;

&lt;p&gt;First, an Intel D435i camera captures the RGB and depth images of the fiber glass sample. A canny 
edge detector is applied to the RGB image to separate the inherit pattern visible underneath the 
surface of the fiber glass from the defects present on the surface. After this step, the surface 
defects become easily visible. To create a general region of the defective area, I use morphological
closing to merge nearby edges and fill openings within a certain area to create a blob like structure
where defects reside. Next I use contour detection to get the pixel locations of everything within
the defective regions.&lt;/p&gt;

&lt;h2 id=&quot;path-planning&quot;&gt;Path Planning&lt;/h2&gt;
&lt;p&gt;Once the locations of the defects are known, a path must be created so the robot can sand over all
of the defective areas. Two different types of path planning were implemented. Multi-goal path planning
was needed to make sure the robot goes to each of the separate defective regions and coverage path
planning was used to ensure the robot sanded all of the defects within a given region.&lt;/p&gt;

&lt;p&gt;For multi-goal path planning, a nearest neighbor algorithm was implemented to create a path that 
went to each region based off of the location of the regions centroid. Next, a grid-based sweeping 
algorithm is used to create a path that covers the entire area. Finally, the two plans are combined
and the robot’s sander is pressed against the piece for the coverage paths to ensure sanding and it 
is offset from the piece during multi-goal movements so that smooth sections are not sanded erroneously.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46512429/208995195-854b8fea-4304-4bf5-8cb5-7975b003edc2.png&quot; height=&quot;425&quot; width=&quot;425&quot; /&gt; &lt;img src=&quot;https://user-images.githubusercontent.com/46512429/208995475-84289c70-9f8f-4b67-a78f-b1ce5674cb63.png&quot; height=&quot;425&quot; width=&quot;425&quot; /&gt; &lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;Example of Finalized Path in 2D and 3D&lt;/p&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;
&lt;p&gt;The system successfully detected the defected areas and we quantified the results by having a person
manually label defects in one of the fiber glass image samples and compared that to what the system
detected. Fifteen fiber glass panels were used and the calculated average sensitivity obtained was 
66.24%, the average specificity was 78.20%, and the resulting accuracy was 81.02%. Furthermore, 
using a profilometer, I measured the surface roughness of certain defected areas before and after 
sanding and found that the average roughness at these areas was about half as rough after performing
autonomous sanding.&lt;/p&gt;

&lt;h2 id=&quot;contributors&quot;&gt;Contributors&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;William Ard&lt;/li&gt;
  &lt;li&gt;Corina Barbalata&lt;/li&gt;
  &lt;li&gt;Joshua Nguyen&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Sun, 22 May 2022 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/icore/</link>
        <guid isPermaLink="true">http://localhost:4000/icore/</guid>
        
        
        <category>Computer Vision</category>
        
        <category>OpenCV</category>
        
        <category>UR5 Robot Arm</category>
        
        <category>Manufacturing</category>
        
        <category>Research</category>
        
        <category>Intel Realsense</category>
        
      </item>
    
      <item>
        <title>Cocktail Maker</title>
        <description>&lt;p&gt;ROS / MoveIt / Python / Computer Vision / Intel Realsense / Franka Emika Panda Arm&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;&lt;iframe width=&quot;424&quot; height=&quot;754&quot; src=&quot;https://www.youtube.com/embed/AqyQ7yTlyfM&quot; title=&quot;cocktail maker&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46512429/209448159-8fa2b899-9e08-43c5-86b6-f411c93f6046.png&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;Electrical Schematic&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46512429/209448153-3b40e4dd-774a-401b-9d44-19efafc30134.png&quot; width=&quot;500&quot; /&gt;&lt;/p&gt;
&lt;p align=&quot;center&quot;&gt;System Pipeline&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;&lt;h2&gt; &lt;a href=&quot;https://github.com/oubrejames/pen_challenge&quot;&gt;View it on Github&lt;/a&gt;&lt;/h2&gt;&lt;/div&gt;
</description>
        <pubDate>Wed, 13 Apr 2022 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/cocktail/</link>
        <guid isPermaLink="true">http://localhost:4000/cocktail/</guid>
        
        
        <category>ROS</category>
        
        <category>MoveIt</category>
        
        <category>Python</category>
        
        <category>Computer Vision</category>
        
        <category>Intel Realsense</category>
        
        <category>Franka Emika Panda Arm</category>
        
        <category>Group Project</category>
        
      </item>
    
      <item>
        <title>FSAE</title>
        <description>&lt;p&gt;Mechanical Design / Mechatronics / PID Control&lt;/p&gt;

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/7S1Z_5BLMUo&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;For my winter quarter independent study project, I set out to build my own version of a BallBot, a dynamically stable / statically unstable robot. I designed my bot from CAD to control algorithm, utilizing 3D printing, laser cutting, and C programming on an RP2040 microcontroller.&lt;/p&gt;

&lt;h3 id=&quot;mechanical-design&quot;&gt;Mechanical Design&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://algarv.github.io/Portfolio/assets/images/ballbot_CAD.png&quot; alt=&quot;CAD&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I designed my chassis with inspiration from similar projects using Solidwork CAD to create 4 part adjustable base. Each motor housing component is attached to the main chassis separately, so the angle can be adjusted, which allowed me to test on various sizes of balls.&lt;/p&gt;

&lt;h3 id=&quot;building-the-structure-and-fundamental-electronics&quot;&gt;Building the Structure and Fundamental Electronics&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://algarv.github.io/Portfolio/assets/images/circuit.png&quot; alt=&quot;Circuit Diagram&quot; /&gt;&lt;/p&gt;

&lt;p&gt;After printing and assembling the chassis pieces, I mounted the 3 motors with 3 motor drivers and a RP2040 ThingPlus microcontroller and started with the basic control of one motor, working on a script to step one wheel at a time.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://algarv.github.io/Portfolio/assets/images/spinning.gif&quot; alt=&quot;Spinning&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;simultaneous-motor-control&quot;&gt;Simultaneous Motor Control&lt;/h3&gt;

&lt;p&gt;The next step was to simultaneously step three motors. This proved to be a more difficult task than anticipated, as it seemed one motor would interfere with the other’s power source. However, after switching to drivers with a higher voltage rating, I was able to step each motor sequentially.&lt;/p&gt;

&lt;p&gt;Once I could send alternating stepping commands to each motor, the base could perform basic directional movement including spinning in place and uniaxial paths (forwards, backwards, sideways).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://algarv.github.io/Portfolio/assets/images/simulatenous_spinning.gif&quot; alt=&quot;Simultaneous Spinning&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://algarv.github.io/Portfolio/assets/images/first_steps.gif&quot; alt=&quot;First Steps&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;twist-commands&quot;&gt;Twist Commands&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://algarv.github.io/Portfolio/assets/images/ballbot_calcs.png&quot; alt=&quot;Calculations&quot; /&gt;
Deriving wheel rotation rate from a twist&lt;/p&gt;

&lt;p&gt;To make a truly omnidirectional base, the next step was to follow any twist, which require stepping the motors at different frequencies. I started with the most basic form of a control algorithm, a polling loop that stepped each motor after a calculated number of sleep cycles resulting in a wheel RPM calculated by the omnidirectional base matrix equation.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://algarv.github.io/Portfolio/assets/images/omnidirectional.gif&quot; alt=&quot;Mobile Base&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;position-control&quot;&gt;Position Control&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://algarv.github.io/Portfolio/assets/images/control_diagram.png&quot; alt=&quot;Control Diagram&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Once the mobile base was capable of following any twist command, it was time to integrate the IMU sensor and create a closed control loop. In my first iteration, only the accelerometer data from the IMU was used to calculate a roll and pitch, which were then used as inputs to the PD controller. The PD controller outputs an x and y velocity relative to the robot’s base frame, which are then converted to wheel speeds and finally step delays using the omnidirectional base equation shown above.&lt;/p&gt;

&lt;h3 id=&quot;self-righting&quot;&gt;Self-Righting&lt;/h3&gt;

&lt;p&gt;The first milestone on the way to balancing was having the robot self-right itself back to a stable position on a ball. During testing, it became apparent that the motors would need to be stepped much faster in order for the robot to catch itself when the ball is not stabilized.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://algarv.github.io/Portfolio/assets/images/self_righting.gif&quot; alt=&quot;Self Righting&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;design-adjustments&quot;&gt;Design Adjustments&lt;/h3&gt;

&lt;p&gt;I redesigned the initial concept and chose to try a much larger ball in order to reduce the angle of lean and therefor the lever arm of the robot’s body over its center of mass. This, along with reducing to half steps, resulted in a smoother self-righting action but the wheel stepping was still prohibitively slow.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://algarv.github.io/Portfolio/assets/images/smoother.gif&quot; alt=&quot;A Little Smoother&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;control-adjustments-and-sensor-tuning&quot;&gt;Control Adjustments and Sensor Tuning&lt;/h3&gt;

&lt;p&gt;I tried several different approaches to the control algorithm once it became apparent that a simple polling loop would not step the motors at the required frequency. When I realized system clock-based interrupts were not support on the rp2040, I attempted 3 separate timers, with each frequency updated after every update from the IMU. While this was successful in spinning the wheels much faster, three timers could not keep up with the step frequency and the reliability of the original polling loop was lost. This was most likely due to the constant resetting of the timers and the competing operations with no set priorities. Finally, I was able to merge the control strategies and create a single timer at a fixed frequency that then counted the number of cycles since the previous step and compared the count to the calculated step delay. This significantly increased the speed of the wheels without compromising a quick and accurate twist response.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://algarv.github.io/Portfolio/assets/images/good_control.gif&quot; alt=&quot;Good Control&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The timer came with a trade off between the IMU measurement frequency and the maximum step frequency, but I found reading the IMU at approximately 10 Hz (a 100 ms delay between measurements) and polling the motors at 500 Hz (a 2 ms timer) was most successful in generating a fast enough reaction time while supporting a high motor stepping frequency.&lt;/p&gt;

&lt;p&gt;With the wheels more reactive, noise in the IMU also became more apparent. I started with a rolling average filter, which worked well to smooth the signal and reduce spikes. The rolling average was effective at slower speeds, but with a much more reactive control loop, I wanted to explore more advanced methods of sensor fusion. A complementary filter allowed me to supplement the noisy accelerometer data with the steadier (but driftier) gyroscope data. This significantly reduced the steady state measurement spikes from up to +/- 40 degree spikes to negligible noise when the bot is stable.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://algarv.github.io/Portfolio/assets/images/complimentary_filter.gif&quot; alt=&quot;Getting Smoother&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://algarv.github.io/Portfolio/assets/images/final.gif&quot; alt=&quot;Last Run&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Ultimately, after carefully tuning the PD gains and tweaking timer frequencies, my ballbot could balance for about 2 seconds before tipping over and losing control. I believe my biggest limitation was the tradeoff between measurement frequency and stepping frequency, and would like to try timer-based interrupts in future iteration or even separating the controller into multiple cores that could run the motor stepping commands and read the IMU simultaneously.&lt;/p&gt;

&lt;p&gt;https://github.com/algarv/Ball_Balancing_Robot/&lt;/p&gt;
</description>
        <pubDate>Sun, 23 May 2021 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/fsae/</link>
        <guid isPermaLink="true">http://localhost:4000/fsae/</guid>
        
        
        <category>Mechatronics</category>
        
        <category>Controls</category>
        
        <category>Solidworks</category>
        
        <category>C</category>
        
      </item>
    
      <item>
        <title>Robotics minor</title>
        <description>&lt;p&gt;Mechatronics, SLAM, Controls, Python, C++&lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;&lt;img src=&quot;https://raw.githubusercontent.com/oubrejames/oubrejames.github.io/gh-pages/assets/images/rob_minor.gif
&quot; alt=&quot;rob_minor_gif&quot; width=&quot;700&quot; /&gt;&lt;/div&gt;

&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;
&lt;p&gt;While at LSU, I completed a minor in robotics engineering alongside my electrical engineering
major. For this minor I took a number of robotics related classes covering a range of topics. Many 
of these classes were very hands on and I got a lot of useful project experience. Two of the 
courses that I found particularly useful were Intro to Robotics Engineering and Autonomous Robotic Vehicles&lt;/p&gt;

&lt;p&gt;In Intro to Robotics Engineering we learned about a broad array of topics from mechanical design to 
software. We learned about robotic actuators and mobility mechanisms, robot motion control, navigation 
and mapping, and human-robot interaction. Particularly, we had many interesting projects involving 
the Parallax Activitybots, hexapods, and Turtlebots. Furthermore, the class focused heavily on learning
ROS and had many labs, classes, and assignments dedicated to learning it.&lt;/p&gt;

&lt;p&gt;Autonomous Robotic Vehicles covered topics like vehicle kinematics, motion control, perception, 
localization, path planning, and navigation. In the class we learned about and implemented different 
probabilistic methods like Particle Filter and Extended Kalman Filter (EKF) for navigation. As our final 
project of the class, we implemented an Extended Kalman Filter (EKF) SLAM using Python and ROS on a Turtlebot 
to navigate through a maze, map it, and localize itself.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;&lt;img src=&quot;https://user-images.githubusercontent.com/46512429/209484859-2579aa0f-0024-4640-8194-f3b06593e2fa.PNG&quot; height=&quot;400&quot; width=&quot;430&quot; /&gt; &lt;img src=&quot;https://user-images.githubusercontent.com/46512429/209484862-2830e017-f679-415b-ab06-1b02ff7585c9.jpg&quot; height=&quot;400&quot; width=&quot;430&quot; /&gt; &lt;/p&gt;

&lt;div align=&quot;center&quot;&gt;&lt;h2&gt; &lt;a href=&quot;https://github.com/oubrejames/slam_ENGR_4200&quot;&gt;View it on Github&lt;/a&gt;&lt;/h2&gt;&lt;/div&gt;

&lt;h2 id=&quot;projects&quot;&gt;Projects&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;ins&gt;&lt;strong&gt;Maze solving robot (leftmost gif)&lt;/strong&gt;&lt;/ins&gt;:  Programmed Parallax Activitybot in C++ to navigate through a 
maze, create a map of the maze, and save the quickest path to the end&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;ins&gt;&lt;strong&gt;Hexapod (middle gif)&lt;/strong&gt;&lt;/ins&gt;:  Explored different gaits and control to move the hexapod in 
particular ways, achieve turning motions, and climb stairs&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;ins&gt;&lt;strong&gt;Increasing Activitybot Payload (rightmost gif)&lt;/strong&gt;&lt;/ins&gt;:  Created an external chassis for the
Activitybot and retrofitted its drivetrain with different gears to increase its torque capacity to 
push heavier objects&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;ins&gt;&lt;strong&gt;Extended Kalman Filter SLAM on Turtlebot&lt;/strong&gt;&lt;/ins&gt;:  Implemented EKF SLAM using Python and ROS 
to navigate through a maze, map it, and localize itself.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;relevant-classes&quot;&gt;Relevant Classes&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Intro to Robotics Engineering&lt;/li&gt;
  &lt;li&gt;Autonomous Robotic Vehicles&lt;/li&gt;
  &lt;li&gt;Sensors and Actuators&lt;/li&gt;
  &lt;li&gt;Intro Computer Vision&lt;/li&gt;
  &lt;li&gt;Adjustable Speed Drives&lt;/li&gt;
  &lt;li&gt;Microprocessor Systems&lt;/li&gt;
  &lt;li&gt;Advanced Control System Design&lt;/li&gt;
  &lt;li&gt;Discrete Control System Design&lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Fri, 21 May 2021 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/a_robotics_minor/</link>
        <guid isPermaLink="true">http://localhost:4000/a_robotics_minor/</guid>
        
        
        <category>Mechatronics</category>
        
        <category>SLAM</category>
        
        <category>Controls</category>
        
        <category>Python</category>
        
        <category>C++</category>
        
      </item>
    
      <item>
        <title>Computer Vision Golf Range Finder</title>
        <description>&lt;p&gt;OpenCV / MediaPipe Pose Estimation&lt;/p&gt;

&lt;h2&gt; Project Goal &lt;/h2&gt;
&lt;p&gt; Detect yoga poses performed by a user and overlay a corresponding icon image. Running the main script starts the videostream with automatic pose detection. &lt;/p&gt;

&lt;h3&gt; Part 1: Pose Detection &lt;/h3&gt;

&lt;p&gt; I use the 32 body landmarks provided by MediaPipe to measure joint angles, then determine yoga poses based on key joint angles for each pose. For example, in the star pose, the angle between the shoulder, elbow, and wrist landmarks (elbow flexion) are below 20 degrees and the angle of the elbow, shoulder, and opposite shoulder (shoulder flexion) are also below 20 degrees. &lt;/p&gt;

&lt;h3&gt; Part 2: Icon Image Transformation &lt;/h3&gt;

&lt;p&gt; To transform the icon image that will be overlayed over the user, I first preprocess the icon image then apply an affine transform. To preprocess the icon, I resize the icon image to be roughly the same heigt as the user, a metric also calculated with MediaPie&apos;s landmarks. I then apply a border to the icon image so that its image array has the same dimensions as the video stream frames. These steps help make the affine transform more effective. I select three key pose landmarks for each pose, then find three key points on the icon that should match these points. For example, I chose to match the nose and ankles of the person with the top tip and bottom two tips of the star. &lt;/p&gt;

&lt;h3&gt; Part 3: Image Overlay &lt;/h3&gt;

&lt;p&gt; I overlayed just the icon pixels (the icon background is ignored) by summing .5 of the icon pixel value with .5 of the the video frame value, resulting in a transparent overlay of just the icon. &lt;/p&gt;

&lt;h2&gt; Results &lt;/h2&gt;

&lt;h3&gt;Star Pose&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://algarv.github.io/Portfolio/assets/images/star.gif&quot; alt=&quot;Star Gif&quot; /&gt;&lt;/p&gt;

&lt;h3&gt;Tree Pose&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://algarv.github.io/Portfolio/assets/images/tree.gif&quot; alt=&quot;Tree Gif&quot; /&gt;&lt;/p&gt;

&lt;h3&gt;Chair pose&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://algarv.github.io/Portfolio/assets/images/chair.gif&quot; alt=&quot;Chair Gif&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Thu, 20 May 2021 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/golf/</link>
        <guid isPermaLink="true">http://localhost:4000/golf/</guid>
        
        
        <category>Computer Vision</category>
        
        <category>Pose Estimation</category>
        
        <category>MediaPipe</category>
        
      </item>
    
  </channel>
</rss>
